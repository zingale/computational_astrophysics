{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two situations where we can imagine needing to compute a derivative:\n",
    "\n",
    "1. We have an analytic function, $f(x)$, and we want to create a\n",
    "numerical approximation to its derivative\n",
    "\n",
    "2. We have a function $f(x)$ defined only at a finite set of (possibly regularly spaced) points, and we want to use that discrete data to estimate the derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first case, it is usually best to take the analytic derivative.  In the previous notebook however, we did look at the effect of roundoff on computing a derivative.\n",
    "\n",
    "We'll focus on the second case here, but not that this can be applied to the case where the function is known analytically provided we evaluate it at a fixed set of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First order approximations\n",
    "Consider a set of points labeled with an index $i$, with the physical spacing between them denoted $\\Delta x$.\n",
    "\n",
    "![discrete data](fd_grid.png)\n",
    "\n",
    "We'll label the function value at $x_i$ as $f_i$, e.g., $f_i = f(x_i)$.\n",
    "\n",
    "We can use the result of the Taylor expansion we previously derived to write the derivative as:\n",
    "\n",
    "$$\\left . \\frac{d f}{dx} \\right |_i = \\frac{f_{i+1} - f_i}{\\Delta x} + \\mathcal{O}(\\Delta x)$$\n",
    "\n",
    "where $f_{i+1} = f(x_{i+1})$ is the data we have at the point $i+1$.  \n",
    "\n",
    "As $\\Delta x \\rightarrow 0$, this approaches the definition of the derivative from calculus.  However, we are not free to choose $\\Delta x$&mdash;it is a property of the discrete set of points we are given.\n",
    "\n",
    "Note: we could alternately have used the point to the right of $i$:\n",
    "\n",
    "$$\\left . \\frac{d f}{dx} \\right |_i = \\frac{f_{i} - f_{i-1}}{\\Delta x} + \\mathcal{O}(\\Delta x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order approximation\n",
    "\n",
    "Looking at the Taylor expansion of $f_{i+1} = f(x_{i+1}) = f(x_i + \\Delta x)$, we see\n",
    "\n",
    "$$f_{i+1} = f_i + \\Delta x \\left .\\frac{df}{dx} \\right |_i + \\frac{1}{2} \\Delta x^2 \\left . \\frac{d^2f}{dx^2} \\right |_i + \\mathcal{O}(\\Delta x^3)$$\n",
    "\n",
    "likewise:\n",
    "\n",
    "$$f_{i-1} = f_i - \\Delta x \\left .\\frac{df}{dx} \\right |_i + \\frac{1}{2} \\Delta x^2 \\left . \\frac{d^2f}{dx^2} \\right |_i + \\mathcal{O}(\\Delta x^3)$$\n",
    "\n",
    "subtracting these two expressions give:\n",
    "\n",
    "$$f_{i+1} - f_{i-1} = 2 \\Delta x \\left . \\frac{df}{dx} \\right |_i + \\mathcal{O}(\\Delta x^3)$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\left . \\frac{df}{dx} \\right |_i = \\frac{f_{i+1} - f_{i-1}}{2 \\Delta x} +\\mathcal{O}(\\Delta x^2)$$\n",
    "\n",
    "This is called the *centered-difference* approximation to the first derivative.  It is second order accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically, these different approximations appear as:\n",
    "![](derivs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider the function $f(x) = \\sin(x)$.  The code below defines 10 equally spaced points and defines $f(x)$ at each point. \n",
    "    \n",
    "Use this discrete data to estimate the derivative at `x[3]` and compute the error with respect to the analytic value.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, np.pi, 10, endpoint=False)\n",
    "f = np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first we'll write functions to evaluate each of the derivative approximations\n",
    "# at a given index idx\n",
    "\n",
    "def left_sided_deriv(x, f, idx):\n",
    "    \"\"\"return the left-sided derivative at x[idx]\"\"\"\n",
    "    return (f[idx] - f[idx-1]) / (x[idx] - x[idx-1])\n",
    "\n",
    "def right_sided_deriv(x, f, idx):\n",
    "    \"\"\"return the right-sided derivative at x[idx]\"\"\"\n",
    "    return (f[idx+1] - f[idx]) / (x[idx+1] - x[idx])\n",
    "\n",
    "def centered_deriv(x, f, idx):\n",
    "    \"\"\"return the left-sided derivative at x[idx]\"\"\"\n",
    "    return (f[idx+1] - f[idx-1]) / (x[idx+1] - x[idx-1])\n",
    "\n",
    "# always use x[ival] for the location of the derivative\n",
    "ival = 3\n",
    "\n",
    "dl = left_sided_deriv(x, f, ival)\n",
    "dr = right_sided_deriv(x, f, ival)\n",
    "dc = centered_deriv(x, f, ival)\n",
    "\n",
    "print(f\"left = {dl}, right = {dr}, centered = {dc}\")\n",
    "print(f\"analytic value = {np.cos(x[ival])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "## Convergence\n",
    "\n",
    "Now we can see if we get the convergence that truncation error suggests.  We'll look at the centered difference.  As we cut $h$ in half, the error should go down by $1/4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0.1 :      0.0009000536984\n",
      "      0.05 :      0.0002250978217\n",
      "     0.025 :      5.627973143e-05\n",
      "    0.0125 :      1.407026263e-05\n",
      "   0.00625 :      3.517586261e-06\n",
      "  0.003125 :      8.793978642e-07\n",
      " 0.0015625 :      2.198495549e-07\n",
      "0.00078125 :      5.496242872e-08\n",
      "0.000390625 :      1.374043401e-08\n",
      "0.000195313 :       3.43500639e-09\n",
      "9.76563e-05 :      8.588626477e-10\n",
      "4.88281e-05 :      2.159635804e-10\n",
      "2.44141e-05 :      5.339140241e-11\n",
      "1.2207e-05 :      1.019040408e-11\n"
     ]
    }
   ],
   "source": [
    "def derivative(x0, f, h):\n",
    "    \"\"\"compute the derivative of the function f(x) at x0 using stepsize h\"\"\"\n",
    "    return (f(x0 + h) - f(x0 - h)) / (2 * h)\n",
    "\n",
    "def func(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def fprime(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "h = 0.1\n",
    "x0 = 1.0\n",
    "while h > 1.e-5:\n",
    "    err = np.abs(derivative(x0, func, h) - fprime(x0))\n",
    "    print(f\"{h:10.6g} : {err:20.10g}\")\n",
    "    \n",
    "    h /= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the expected 2nd order convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
